\section{Overview}

\par Solving complex optimization problems has been a hot topic the past decade and has garnered large followings most of which are practitioners and researchers. This attention has brought upon the development of new metaheuristic algorithms, many of which are inspired by various phenomena demonstrated by nature.
\par The paper proposes a new population based algorithm, the Lion Optimization Algorithm (LOA) \cite{loa_2016}. The distinct lifestyle of lions and their characteristics of utilizing cooperation was made as the motivational basis for the development of this optimization algorithm. The algorithm is also tested against benchmark problems sourced from literature and whose primary solutions was compared with the results of the test. The results also confirm the performance of this algorithm alongside other algorithms used in the paper.

\section{Optimization}
\par Basically, optimization is searching for the best solutions out of all possible solutions. A best solution can be defined regarding either the most of some measure of success (e.g. revenue) or the least of another measure (e.g. cost). We can be looking at a group of answers or one answer from a set. In solving optimization problems, one's goal is to minimize or maximize a result variable of a function by trying out different parameters for input. \par

\section{Optimization Algorithms}
\par Optimization algorithms can be divided into two major categories, as exact and approximate \cite{desale_2015}. Exact algorithms guarantee that the optimal solution to the problem will be found in a finite amount of time. There are, however, harder optimization problems that requires the searching of very large solution sets and thus making it impractical to use exact algorithms. An example of this is the travelling salesman problem, whereby a salesman is tasked to plan a path that visits a series of cities exactly once and return to his starting point with minimum distance travelled.

\par In this problem, the number of possible paths that the salesman can take grows factorially as the number of cities increases. It is possible to solve this problem using brute-force method but it would take a lot of time. As such, the usage of approximate algorithms are necessitated. These algorithms do not guarantee that the optimal solution will be found, but it can find an approximate (sometimes exact) solution to the problem in a relatively short amount of time, sometimes using a less computationally intensive method. Approximate algorithms can be further divided into two major categories as heuristic and metaheuristic algorithms \cite{desale_2015}.

\par Heuristic algorithms are problem-dependent techniques that approximate the solution to a problem using readily available information. Meaning, they try to take advantage of the particularities of the problem to find a solution. Applications of these algorithms include finding the best move in a chess game, solving a tic-tac-toe puzzle, and pathfinding. In these examples, the underlying concepts of the problem are first analyzed and then used to guide the algorithm in searching for a solution.

\par Metaheuristic algorithms, on the other hand, requires minimal or no assumptions about the problem being solved. They can be tailored to optimize a specific problem which makes them applicable to a wide variety of problems. A metaheuristic algorithm optimizes a problem by iteratively improving a candidate solution until a desired quality is achieved. Metaheuristic algorithms often employ mechanisms to escape from being stuck in a local optimum and thus making them more likely to obtain the global optimum solution.

\par An example of an algorithm that can often get stuck in a local optimum is the hill climbing algorithm. The hill-climbing algorithm starts with an arbitrary solution and makes iterative changes to it. If, after an iteration, the current solution state is found to be worse than its previous state, then it is reverted back. Otherwise, the solution is retained and is changed again. This process is repeated until no further improvements can be done to the solution.
\par The problem with this method is that it is greedy, meaning that the solution achieved can be the best solution out of all its neighbouring solution, but not out of the entire solution space. This can be visualized by considering all possible states of the solution laid out on the surface of a landscape, wherein the height of any point on the landscape represents the optimality of the solution state at that point.
\par Say we have two hills on that landscape, one of them higher than the other. The higher hill represents the global optimum of the solution space, while the lower hill represents the local optimum. If the initial solution state is near the smaller hill, it is more likely that the algorithm will "climb" the smaller hill. And since, it only accepts a state that is better than the previous one, it will not be able to go down the hill and climb the higher hill. Thus, it can get stuck in that local optimum.\par
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/intro/hill_climbing}
\caption{Hill-climbing algorithm local optimum}
\end{center}
\end{figure}
\par Modern algorithms have mechanisms that tackle this problem. An example of an algorithm that employ such mechanisms is simulated annealing. The simulated annealing is very similar to hill-climbing except that it occassionally accepts solutions that are worse than the current. Over time, the probability of accepting such solutions decreases. This allows for the avoidance of getting stuck in a local optimum.

\par The simulated annealing algorithm is modeled after the cooling process of a molten substance, and is one of many algorithms based on natural processes. Others include the genetic algorithm, ant colony optimization, and particle swarm optimization. For centuries humans have relied on nature to find the most appropriate solutions to problems. That's why in the past decades, computer scientists has turned to nature to develop novel algorithms.

\section{No Free Lunch Theorems}
\par Since there are already so many optimization algorithms, what's the point of making a new one? The No Free Lunch Theorems were introduced in 1997 by Wolpert and Macready to address the need for newer optimization algorithms. The first of their theorems states that for any pair of algorithms $a_1$ and $a_2$, iterated $m$ times, $$ \sum_{f} P(d_{m}^{y}|f,m,a_1)=\sum_{f} P(d_{m}^{y}|f,m,a_2),$$ where $d_m^y$ denotes the set of size $m$ of the cost values $y\in Y$ associated to the input values $x\in X,f:X\to Y$ is the function being optimized and $P(d_m^y|f,m,a)$ is the conditional probability of obtaining a given sequence of cost values from algorithm $a$ run $m$ times on function $f$. Essentially, this says that when all functions $f$ are equally likely to be used, the probability of observing an arbitrary sequence of cost values over the course of optimization is independent of the algorithm.

\par This theorem indicates that if an algorithm performs better than another algorithm on some class of problems, then it must perform worse on the remaining problems. Consequently, this implies that there is no general-purpose optimization algorithm that is universally superior than the rest. Hence, the development of newer optimization algorithms is still needed.

\section{Optimization: Genetic Algorithm}
\par The genetic algorithm \cite{holland_2010} is modeled after Darwinâ€™s theory of evolution. An initial population is initialized in which each individual represents a solution to the problem. Through a series of selection, crossover, and mutation, successive populations are generated until an individual with a desired fitness is obtained. Fitness is defined as how well an individual can solve the given problem.

\par Selection pertains to the process in which individuals with the best fitness are selected and allowed to pass their genes (or properties. E.g. bits) to the next generation. The individuals with the highest fitness have a higher chance of being selected.

\par Crossover is when genes of the selected individuals are interchanged with each other. The results are called offspring and are added to the next generation.

\par Mutation refers to when genes of the offspring are subject to mutate or change (e.g. Binary value from 1 to 0). This further diversifies the sample space and consequently prevents the algorithm from converging early.

\section{Optimization: Particle Swarm Optimization}
\par The Particle Swarm Optimization \cite{eberhart_kennedy_1995} algorithm is modeled after the movement of a swarm as a whole (e.g. A flock
of birds collectively foraging for food).

\par In this algorithm, an initial set of particles is first initialized with each particle having a random initial velocity. These particles are then flung through hyperspace where each of their position
represents a solution to the problem at hand.

Each of these solutions are evaluated regarding their fitness and a particle's
current most fit solution is stored as its `pbest'. The current best solution attained by the entire set particles is also stored; this is called `gbest'. For each time step of the algorithm, each particle is accelerated towards its `pbest' and the `gbest'. Eventually, these particles will converge around an optimal solution.

\section{Optimization: Artificial Immune Systems}
\par Artificial immune systems are a classification of rule based machine learning systems inspired by the vertebrate immune systems. This systems model the learning and memory for use in problem solving. These systems adapt to what they learn in the environment to become better at solving problems. Compared to GA these methods use less mutation per generation whenever the fitness becomes better.

\section{Optimization: Ant Colony Optimization}
\par Ants can find food faster by utilizing shorter paths found by the other ants in its colony. An ant would leave more pheromones when it has reached the food faster so other ants would be more inclined to use the path. This also applies to optimization by using memory and prioritizing directions with more incentives.

\section{Optimization: Marriage in Honey Bee Optimization}
\par The model simulates the evolution of honey-bees starting with a solitary colony (single queen without a family) to the emergence of an eusocial colony (one or more queens with a family).

\section{Optimization: Lion Pride Optimizer}
\par Previous works such as the Lion Pride Optimizer was inspired by this brutal competition of male lions whom also plays an important role for the persistence of the pride. In the work, the optimization chooses two of the best points in a ``pride'' and each ``female'' with a mating coefficient will create 4 offsprings only to choose one at last based on the best male in the pride. The optimization also uses safeguards to prevent stagnation in the pride by either replacing all members in the pride or resetting the search space.

\section{Optimization: Lion's Algorithm}
\par The lion's algorithm by Rajakumar \cite{rajakumar_2012} is another inspiration for the LOA. This algorithm is modeled after the territorial behavior of a lion pride, where the pride represents the solution space and a lion represents a solution. The pride is first initialized with one male and one female lion. Through mating, four cubs are generated as a result of single point crossover with dual possibilities. Four more cubs are generated from the mutation of these cubs, totalling eight cubs. These cubs are then grouped according to gender, and the weakest cubs are killed. A cub needs 2-4 years to reach maturity and so the territorial lions must defend the territory for the same number of years.
\par During this time, nomadic lions may invade the pride. For each year, a nomadic lion is generated to test the strength of the pride. If the nomadic lion is found to be stronger than the territorial lions, the nomadic lion takes over the pride and kills the territorial lions' cubs. If the cubs survive and they mature, the best male and female lions take over the entire pride while the rest are killed.

\section{Applications}
\par Researchers have made good use of optimization problems such as scheduling problems, data clustering, image and video processing, tuning of neural networks, and pattern recognition.

\section{Application: Scheduling Problems}
\par Scheduling problems are problems where different difficulty on jobs would take different amount of time that will be processed by different nodes. The problem is to minimize the time that all the nodes would simultaneously get to be finished. Optimization helps find the best job to node assignment to minimize the mean time.

\section{Application: Data Clustering}
\par Data clustering or cluster analysis is the way to group a set of objects such that objects with the most similarity a group together in clusters. The objects may have one or more properties to identify and the groups may have smaller subgroups that one can also classify. Optimization helps identify the best clustering of an object based on its parameters.

\section{Application: Image and Video Processing}
\par Image and video processing is the process of adding metadata to an image or video based to what its visual content actually is. An image or video in a computer is represented as pixels or boxes of colors that is displayed on the screen of the viewer. These pixels individually cannot determine the actual content, which is significant to the viewer, of the image and video.

\par Optimization algorithms help computers identify what pixels in an image or video actually represents to the viewer. Such algorithms help with edge detection, segmentation, representation and description of the parts of an image or video.

\section{Application: Tuning of neural networks}
\par Neural networks can be tuned by its parameters. These parameters talked about are parameters that are constant throughout the run of the network. This parameters may be tuned to get the best performance out of neural network which may also drive the network to learn faster, slower or not at all. Optimization helps to find the best parameters that will drive the system's performance.

\section{Application: Pattern Recognition}
\par Patterns can be also found in data. These patterns can help add to the metadata of that data. Pattern recognition finds those patterns that can be found off the data. Optimization helps identify patterns that best identify a given data.

\section{Inspiration for the algorithm}
\par Lions have displayed cooperation and antagonism especially in hunting. Lions are also socially inclined meaning that they also organize information that other lions have collected and use them for their benefit.
Male lions have radically different social behavior and appearance than the female lions and v.v.
\par The lions can also be classified if they're residents or nomads. Resident lions create groups called prides, establish their territories and flourish there while nomads take what they need in an area then finds another area to pillage not establishing territories.
\par A pride typically would include five females have cubs of both sexes and one or more adult male lions.
As young males would grow they would separate from their birth pride and establish their own prides.
\par Nomads, who doesn't establish territories, would move about sporadically (whenever they want) and either in pairs or singularly.
Lions usually hunt together in prides. Female lions would work together to surround and swiftly catch the prey. There could also be a hunter female lion who would go out of territory to hunt on their own while the other members of the pride would wait for the lioness to return. But still, coordinated group hunting would bring greater success in prey hunts.\par
\par Lions do go mate anytime around the year and females can have more than one reproductive cycle each year. A lioness can also mate with more than one lion when in heat.
Additionally, to mark their territory the pride would place urine all over the place to drive away others who would intrude.

\section{Idea for the algorithm}
\par The initially proposed algorithm started an initial population formed by a set of solutions randomly generated labelled as Lions.A percentage $\%N$ of the initial population of solutions are selected as `Nomad Lions' while the rest are the `Resident Lions'. While the nomad lions are individually grouped, the resident lions are then further divided into partitions called `Prides' where a percentage $\%S$ is percentage of the females in the group but in nomad lions, this percentage is reversed, where $\%S$ will be used to identify the males in the nomad lions.
\par Each lion will have a variable pertaining to the best obtained solution for every passing iteration that will be called best visited position and will be updated regularly for every iteration. In each pride, a few random females will be selected to go hunting. These females will encircle the prey and catch it. The males in the pride will roam the territory. The females may mate with one or more resident males then a young male is created. These males may establish their own prides and territory later or may become a nomad.
\par The nomad lions roams around the search space to find better (places) solutions. A nomad lion may invade and replace a resident male in a pride, driving out that resident male (replacement). Also, a female lion may also migrate to another pride or become a nomad herself. Weak lions, who have not found better prey (solutions) where there is no competition, will die or be killed (stagnation). The process will go on until the stopping condition is satisfied.

\section{Opposition Based Learning}
\par Opposition based learning is a novel idea of using opposite entities to arrive to better solutions or to arrive at the best solution faster. OBL is used to arrive to the best solution faster than the naive method. In Genetic Algorithm, OBL is also used to get better solutions by allowing multiple best solutions determined using the cost or revenue function to try predict better solutions influenced by the selected solutions' genes.\par

\par A basic example of OBL is finding a solution $X$ in a one dimensional solution set. For the solution $\hat{X}$, we have an estimate $\hat{X}$ that approaches $X$ from the left. As we optimize to the solution $X$, the difference between the solution $X$ and the estimate $\hat{X}$ will get less and less as $\hat{X}$ gets near to the solution $X$. Suppose we get to $X$ at a certain time, which is the time from the start of the optimization to whenever the estimate $\hat{X}$ will be equal to the solution $ X $. If we use OBL and add another estimate $\hat{X}_2$ that would approach $X$ from the right then the time it takes for the optimization to arrive at solution $X$ would be the maximum between the time it takes for $\hat{X}$ or $\hat{X}_2$ to reach solution $X$.

\section{Proposed Algorithm: Initialization}
\par The first step of the algorithm is to randomly generate solutions called Lions with a population of N. In a Nvar dimensional search space optimization problem a Lion is represented as
$$Lion = [x_1, x_2,..., x_{Nvar}]$$
\par As most optimization algorithms, there will be a given cost function, where the fitness value of a solution can be gauged.
$$\text{fitness} = f(\text{Lion}) = f(x_1, x_2,..., x_{Nvar})$$
\par Along with generating the solutions, a percentage of $N$ will be selected as nomad lions and the rest would be divided into a number $P$ of prides. The solutions in the pride will have a specific gender which will identify their role in finding solutions. A percentage $S$ of the prides in the population are labeled as females (others are males) while in nomads will have the ratio reversed where $1-S$ will be the percentage of females in the nomads. The percentage $\%S$ is typically chosen between 75 to 90 percent.

\par The positions of every lion (solution) is random of which is uniformally distributed across the entire search space. Until the percentage of nomad reaches the percentage threshold, lions would be labeled as nomads. The remaining lions would then be labeled as pride lions and would be divided using a specific group size. Each pride lion would be added to the group until the desired group sized would be reached, proceeding to the next group if filled. Until the gender percentage threshold is reached the lions in the pride will be labeled as female otherwise as a male. The same shall be done in labeling the gender of the nomads.

\section{Proposed Algorithm: Hunting}
\par In a pride, females would look for a prey to provide food for the pride. The females would fill specific roles to execute certain strategies to encircle the prey and catch it. In general, lions would approximately follow a pattern in hunting. Stander divided lions into seven different stalking roles which would be grouped into by the Left Wings, Centers and Right Wings.
Left Wings and Right Wings both attack the prey from opposite directions in which the idea of Opposition Based Learning is utilized which also is proven to effectively solve optimization problems by letting solutions escape from local optima.
\par The group with members of the highest finesses (highest revenue, lowest cost) is considered as the Centers while the other two groups are considered as Left and Right Wings.
\par In the algorithm, the hunters are divided into three groups: Left, Right and Center. The group with highest cumulative finesses is considered to be the Center and the other groups would be Left and Right Wings.
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/pa/hunting_scheme}
\caption{Generalized lion hunting behavior}
\end{center}
\end{figure}
A dummy prey would move to a new position and escape as follows:
$$\text{PREY}' = \text{PREY} + \text{rand}(0,1) \times \text{PI} \times (\text{PREY} - \text{Hunter})$$
where PREY is the current position of the prey, PREY$'$ is the new position and PI is the percentage of improvement of the finesse of the hunter.
For the left and right wings, they approach the prey as follows:
\[ \text{Hunter}' =  \begin{cases}
      \text{rand}((2 \times \text{PREY} - \text{Hunter}), \text{PREY}) & (2 \times \text{PREY} - \text{Hunter}) < \text{PREY} \\
      \text{rand}(\text{PREY}, (2 \times \text{PREY} - \text{Hunter})) & (2 \times \text{PREY} - \text{Hunter}) > \text{PREY}
   \end{cases}
\]
where Hunter is the current position of the hunter and Hunter' is the new position of the hunter.
\par As for the Center hunters, their new position is as follows:
\[ \text{Hunter}' =  \begin{cases}
      \text{rand}(\text{Hunter}, \text{PREY}) & \text{Hunter} < \text{PREY} \\
      \text{rand}(\text{PREY}, \text{Hunter}) & \text{Hunter} > \text{PREY}
   \end{cases}
\]
In all of these equations, $\text{rand}(a,b)$ generates a random number between $a$ and $b$, where $a$ and $b$ are upper and lower bounds, respectively.

\par The following figures, figure 3 and figure 4, visualizes how the wing hunter's vector and center hunter's vector changes based on the prey's changing position, respectively. It also shows how the prey moves upon this change of position by the hunter that is currently attacking it in the current iteration. The figure gives an instance of a prey and hunter vector and shows the derivation of positions.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/pa/wing-hunt}
\caption{Wing Hunt Vector Visualization}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/pa/center-hunt}
\caption{Center Hunt Vector Visualization}
\end{center}
\end{figure}

\par In the algorithm, this prey would be a dummy prey put at the start of the phase where PREY would be a random point in the search space. Each of the hunters would be iterated to attack the dummy prey and the prey would escape to the direction opposite to the new position of the attacking Hunter from the Prey's current position.
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/pa/hunting_attack}
\caption{Attack and Escape Example}
\end{center}
\end{figure}
\par This mechanism allows the hunters to create a circle shaped neighborhood around the prey, approach it from different directions. This also allows better solutions to be found (escape local optima) and to avoid premature optimization.
Therefore hunting in each pride can be stated in the following Pseudo-code:
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/pa/hunting_pseudo}
\caption{Hunting Algorithm}
\end{center}
\end{figure}

\par At the beginning of the hunting phase, a group of female lions would be chosen from the pride. These lions would be ranked by their fitness and a number of these lions would be marked as centers while others would be marked as left and right wings.

\par The number of centers chosen for each pride is a third of the hunting population. The remaining pride members would be defaulted to be the left and right wings. Left and right wing lions are identified in the algorithm as lions that are not center. This is because the left and right wings have the same behavior such that it won't matter to identify lefts from the rights.

\par From here, the position for every hunting lion is only changed and the best of each lion is still stored and determined separately.

\section{Proposed Algorithm: Moving towards safe place}

\par As mentioned earlier, some of the females go hunting, not all. So, remaining females go toward one of the areas of the territory. In the algorithm, the territories of each pride would consist of personal best solutions so far, which would assist the algorithm to save the best solutions obtained so far over the course of iteration.
Therefore the new position for a female lion is given as:
\begin{align*}
\text{Lion}' &= \text{Lion} + 2D \times rand(0,1){R1} + U(-1,1) \times \tan(\theta) \times D \times {R2} \\
&\text{  where } R1 \cdot R2 = 0, ||R2|| = 1
\end{align*}
where Lion and Lion' is the previous and next position of the female lion, respectively, and D is the distance between the female lion's position and the selected point chosen by tournament selection in the pride's territory. The following figure shows the range of possible next positions of the lion.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/moving/moving}
\caption{Range of next positions relative to original}
\end{center}
\end{figure}

\par {R1} is a vector which its start point is the previous location of the female lion and its direction is toward the selected position. {R2} is perpendicular to {R1}. $\theta$ is an angle that is selected by uniform distribution among $-\pi/6$ and $\pi/6$ and $U$ is a function selecting a random number with uniform distribution.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/moving/move-out-legend}
\caption{Trajectory visualization by generating 300 outputs}
\end{center}
\end{figure}

\par This mechanism, which would also be used by the resident males for roaming within territory, is also part of the next section.

\par In the following example (which uses the same fitness function and method as the previous section), multiple lions are created in the space; one of which lions is selected to move (source), tournament selection is done without replacement and each selected lion would be evaluated which of them have the best fitness (in this case: least cost).

\par In the algorithm, the tournament selection's size is not set but is computed according to the pride's number of successes in updating their best position in the last iteration. This is computed using the following equations.

\begin{align*}
    K_j(P) &= \Sigma_{i=1}^N  S(i,t) \\
    T_j^{Size} &= \text{max}\left(2, ceil\left(\frac{K_j(P)}{2}\right)\right)
\end{align*}


where, $K_j$ is the number of successes in a pride and $T_j$ is the tournament size of that pride for the next iteration.

\section{Proposed Algorithm: Roaming}
\par Roaming in territory is something lions in a pride usually do. This also helps in getting used to the territory for the lions. For nomads, this is done to find better places to get resources from. In optimization, this can be used to further optimize the local search space for each pride and is used by nomads for searching solutions.

\par Along with roaming, if a resident male visits a new position better than its current position, update its current best position and best visited solution.

\par For pride lions, roaming would mean to use each others' position to find better positions (solutions). Similar to the previous section, in this part of the algorithm, the lions would go towards each other using the same formula. The algorithm for resident male roaming is done by the following pseudo-code:

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/roam/resident}
\caption{Resident Male Roaming Algorithm}
\end{center}
\end{figure}

\par For nomad lions, however, roaming would be utilizing random movement to find better positions (solutions). In the algorithm, nomad lions have an advantage to not get stuck in local optima. A nomad lion would randomly roam around the search if space especially when its fitness among the nomads is worse. These nomad lions are only required to check their standing among peer nomads unlike resident lions whom tries to optimize with each others position, thus, giving the nomad lions an advantage to not get stuck. The new position of these nomads are generated using the following equation:

\begin{align*}
 \text{Lion'}_{ij} =
  \begin{cases}
   \text{Lion}_{ij}        & \text{if rand}(0,1)  > pr_i \\
   \text{RAND}_j        & \text{otherwise}
 \end{cases}
\end{align*}

where Lion$_{i}$ is the position of the lion, j is the dimension, rand is a uniformally distributed random number between 0 and 1 and RAND$_j$ is a random position in the search space. Now, $pr_i$ ($i$ for every nomad lion) is represented by:

$$
pr_i = 0.1 + \text{min}\left(0.5, \frac{ (\text{Nomad}_i - \text{Best}_{nomad}) }{ \text{Best}_{nomad} }\right)
$$

This shows that the nomad lion, instead of moving relative to other positions, would move in a sporadic manner proving its `randomness'. The 0.1 offset would still allow some nomads to reset positions when all nomad lions have been stuck in an optima.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/roam/nomad}
\caption{Nomad Male Roaming Algorithm}
\end{center}
\end{figure}

Similar to the previous examples, the following example simulates the roaming mechanism of the algorithm (using the same parameters) to show how this part of the algorithm helps in optimizing solutions. The mini-algorithm runs until the maximum set iterations of 200. (Iterations on next page.)

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.65\textwidth]{img/roam/roam-init}
\caption{Initial positions and best of nomad and resident lions}
\end{center}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/roam/roam-iter-25}
    \caption{Iteration 25}
    \label{fig:roam-iter-0}
  \end{subfigure}
  %
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/roam/roam-iter-50}
    \caption{Iteration 50}
    \label{fig:roam-iter-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/roam/roam-iter-75}
    \caption{Iteration 75}
    \label{fig:roam-iter-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/roam/roam-iter-100}
    \caption{Iteration 100}
    \label{fig:roam-iter-3}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/roam/roam-iter-125}
    \caption{Iteration 125}
    \label{fig:roam-iter-4}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/roam/roam-iter-150}
    \caption{Iteration 150}
    \label{fig:roam-iter-5}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/roam/roam-iter-175}
    \caption{Iteration 175}
    \label{fig:roam-iter-6}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/roam/roam-iter-200}
    \caption{Iteration 200}
    \label{fig:roam-iter-7}
  \end{subfigure}
  \caption{Roaming Simulation}
\end{figure}

The example shows how fast the optimization technique used in pride lions compared to the slow but sure optimization technique of nomads. Though after 200 iterations, the nomad lions still hasn't optimized enough to the global minima of $(1,1)$.

\section{Proposed Algorithm: Mating and Equilibrium}
Mating of the lions in the population refers to the mixing of their genes to allow exchanging of information. In this algorithm, for every pride, $\%Ma$ of the females mate with one or more males. For the nomads, $\%Ma$ of the females mate with only one male. These individuals are selected randomly. After mating, two offsprings are produced using the equations:

$$\text{Offspring}_j\text{1}=\beta \times \text{Female Lion}_j +\sum\frac{1-\beta}{\sum_{i=1}^{NR}S_i}\times\text{MaleLion}_j^i\times S_i,$$

$$\text{Offspring}_j\text{2}=(1-\beta) \times \text{Female Lion}_j +\sum\frac{\beta}{\sum_{i=1}^{NR}S_i}\times\text{MaleLion}_j^i\times S_i$$,

where $j$ is dimension; $S_i$ is 1 if male $i$ is selected for mating, 0 otherwise; $NR$ is the number of resident males in a pride; $\beta$ is a randomly generated number with a normal distribution with mean value 0.5 and standard deviation 0.1.

One of these offsprings is set as male, and the other as female. A mutation is applied on each gene of one of the produced offspring with probability $Mu$. Through mutation, a random number replaces the value of the gene.

The purpose of $\beta$ in the equation is to find a value somewhere in between the values of the female and male genes; and the purpose of mutation is to further diversify the solution generated.
\\For example:
\\Given the population:

\begin{table}[h!]
\centering
\begin{tabular}{l c l c l c l c l}
\hline
  & 1          & 2        & 3          & 4        \\
\hline
Position    & (0.5, 0.5) & (0.5, 1) & (1.5, 0.5) & (1, 1.5) \\
\hline
\end{tabular}
\caption{Mating Demo Female Population}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{l c l c l c l c l}
\hline
  & 1          & 2        & 3          & 4        \\
\hline
Position    & (0.75, 0.5) & (1, 0.5) & (0.5, 1.5) & (1, 1) \\
\hline
\end{tabular}
\caption{Mating Demo Male Population}
\end{table}

Selecting Female 2 (0.5, 1.0) and Males 2 (0.75, 0.5) and 3 (0.5, 1.5) and $\beta$ as 0.4, the genes are computed as:

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
  \begin{center}
    \includegraphics[width=\textwidth]{img/mating/matingdemooff1}
    \caption{Mating Demo Offspring Gene 1}
  \end{center}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
  \begin{center}
    \includegraphics[width=\textwidth]{img/mating/matingdemooff2}
    \caption{Mating Demo Offspring Gene 2}
  \end{center}
  \end{subfigure}
\end{figure}

The result offspring are Offspring 1 = (0.65, 1) and Offspring 2 = (0.6, 1). Plotting this example yields:
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mating/matingdemooutput}
\caption{Mating Demo Plot}
\end{center}
\end{figure}

\section{Proposed Algorithm: Defense}
When they mature, male cubs in a pride fight with the existing males. The weaker males are driven out of the pride and become nomads. This is done in the algorithm by sorting all the males in the pride according to their fitness. The number of males to be driven out is determined by the sex rate of the pride.

Similarly, nomad males can fight existing males in a pride, and if they prove to be stronger than the resident male, they replace the beaten male in the pride. For each nomad male in the population, it will attack a single or multiple prides determined randomly. Once a pride is selected, the fitness of the nomad male is compared to each resident male of the pride. If the nomad is determined to be stronger, it will replace the resident male, and the resident male becomes a nomad. This behaviour allows the retention of strong lions/solutions in the pride.

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/defense/defense}
\caption{(a) Defense against newly matured cubs and (b) Defense against nomads\cite{loa_2016}}
\end{center}
\end{figure}

\section{Proposed Algorithm: Migration}
Some females in a pride migrate to another pride or become nomads. Through migration, nomad females can also become residents of a pride. This enhances the diversity of the pride by the best position attained by the migrating females. For each pride, the maximum number of females is determined by the sex rate of the pride. The surplus females plus $\%I$ of the maximum number of females in a pride migrate and become nomads. The nomad females are then sorted according to their fitness and the stronger ones are distributed to random prides to fill the empty place of the migrated females. The number of females to be distributed is determined by the amount of empty female slots of all prides. This procedure allows information sharing among the different prides.

\section{Proposed Algorithm: Equilibrium}
At the end of each iteration of LOA, the number of nomad lions will be controlled with respect to the permitted number of each gender determined by the sex rate. The ones with the least fitness value will be removed from the population. This is to maintain equilibrium in the lion's population.

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/equilibrium/equilibrium}
\caption{Migration and Population Equilibrium \cite{loa_2016}}
\end{center}
\end{figure}

\section{Mating, Defense, Migration, and Equilibrium Algorithm Demo}
In this section, the mating, defense, migration, and equilibrium algorithm is demonstrated using an example. The function $$f(x,y)=(x-1)^2 + (y-1)^2$$ is being minimized and the following figure displays the initial positions of the lions.
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/init}
\caption{Mating, Defense, Migration, and Equilibrium Algorithm Demo: Initial positions\cite{loa_2016}}
\end{center}
\end{figure}
The asterisks represent the residential lions and the plus signs represent the nomad lions. The red color means its a female, while the blue color means its a male. The first iteration is as follows:

Through mating, four new lions are generated. The positions of these lions are found in between the positions of their parents. No mutation has occured in this particular example.

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/mating1}
\caption{Iteration 1: Mating - Four new points (lions) are generated}
\end{center}
\end{figure}

The newly generated cubs mature and compete with other residential lions to see who will retain in the pride. In this iteration, a residential male becomes a nomad (indicated by a blue asterisk becoming a blue plus).

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/defense1}
\caption{Iteration 1: Defense - A residential male becomes a nomad}
\end{center}
\end{figure}

Through the migration of residential females, some residential females become nomads, and some nomad females become included in prides (indicated by the red asterisks becoming red pluses and vice versa).

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/migration1}
\caption{Iteration 1: Migration - Some residential females become nomads}
\end{center}
\end{figure}

At the end of the iteration, the population of the lions are controlled with respect to the given sex rate. In this example, two male and female nomads were removed from the population.

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/equilibrium1}
\caption{Iteration 1: Equilibrium - Four lions are killed.}
\end{center}
\end{figure}

The second iteration of the example is as follows:
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/mating2}
\caption{Iteration 2: Mating - Four new points (lions) are generated}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/defense2}
\caption{Iteration 2: Defense - A residential male becomes a nomad and a nomad becomes a residential lion}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/migration2}
\caption{Iteration 2: Migration - Some residential females become nomads}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/equilibrium2}
\caption{Iteration 2: Equilibrium - Four lions are killed}
\end{center}
\end{figure}

For the subsequent iterations we have the following figures:
\begin{figure}[H]
\begin{subfigure}[b]{0.45\textwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/equilibrium3}
\caption{Iteration 3: Equilibrium}
\end{center}
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{img/mdme/equilibrium5}
\caption{Iteration 5: Equilibrium}
\end{center}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/equilibrium10}
\caption{Iteration 10: Equilibrium}
\end{center}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/equilibrium25}
\caption{Iteration 25: Equilibrium}
\end{center}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\begin{center}
\includegraphics[width=0.7\textwidth]{img/mdme/equilibrium50}
\caption{Iteration 50: Equilibrium}
\end{center}
\end{subfigure}
\caption{Iterations 3, 5, 10, 25, and 50}
\end{figure}

After each iteration, the algorithm converges towards the optima of the function. In this case, the minimum of the function is (1,1). In each iteration, the positions of the newly generated lions become progressively closer to this minimum. In the final iteration, the entire population converges at the minimum.

\section{The algorithm}

Using the previous ideas, the algorithm is then created. Using the concepts in the previous section, the pseudocode of the algorithm is presented:

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.95\textwidth]{img/pseudocode}
\caption{The psedocode of LOA}
\end{center}
\end{figure}

The algorithm starts with an initialization. It produces the random samples to be used as test solutions as positions to be improved upon. These samples are grouped in prides or by the nomads. They are also given a gender in these groups.

\par The algorithm then proceeds to the iterative loop that begins with pride hunting, where select females would improve their own positions. The other females in the pride, lions that are not selected in the previous part, would try to find better positions by utilizing the best positions in their pride. The other resident lions, the males, would also do something similar.

\par Better solutions are then made by mating the best lions in a pride. The children from this part is only added to the pride after the mating part has happened. The weakest males that are the least fit are driven out from prides and are put into the nomad group. After that the nomads are then checked and moved randomly in search space. The lions that are nomads are then mated one to one, also creating new solutions and improving on the other positions.

\par The nomad males then try to attack prides by testing the resident male's fitness against their own. After this, the resident females, females in the pride, immigrate bringing them out of the pride and becoming a nomad. The nomad lions' population is then regulated by bringing the very best females back into prides filling the empty space that was once filled from the very initialization of the algorithm.

\par To limit the population, the nomad lions with the least fitness are deleted based on the limit placed using the original population size of the nomads.

\section{Sample Runs using different Benchmark Functions}
\par The algorithm is now tested upon different benchmarking functions. There are 8 used functions to test the effectiveness of the algorithm, 4 of which are used again run in another solution space. All benchmarking function will also have the function plot for 1 dimensional or the function surface for 2 dimensional functions included in the plot. The following functions are overlapped to the corresponding plot or surface beneath the lions' scatter plot (some lions may have their plot point covered behind the 3D surface). All benchmarks ran using the following parameters of the algorithm.
\begin{align*}
\text{Iterations} &= 50\\
\text{Population} &= 50\\
\text{Number of Prides} &= 4\\
\text{Percent of Nomad Lions} &= 0.2\\
\text{Roaming Percent} &= 0.2\\
\text{Mutate Probability} &= 0.2\\
\text{Sex Rate} &= 0.8\\
\text{Mating Probability} &= 0.3\\
\text{Immigrate Rate} &= 0.4\\
\end{align*}
\par The first benchmarking function is:

$$
  g1=f(x) = x^2
$$

and figure 28 shows the plots of the positions on the x-axis with the y-space as function evaluations.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/circ/loa-iter-0}
    \caption{Iteration 0}
    \label{fig:s1-iter-0}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/circ/loa-iter-7}
    \caption{Iteration 7}
    \label{fig:s1-iter-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/circ/loa-iter-14}
    \caption{Iteration 14}
    \label{fig:s1-iter-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/circ/loa-iter-21}
    \caption{Iteration 21}
    \label{fig:s1-iter-3}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/circ/loa-iter-28}
    \caption{Iteration 28}
    \label{fig:s1-iter-4}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/circ/loa-iter-35}
    \caption{Iteration 35}
    \label{fig:s1-iter-5}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/circ/loa-iter-42}
    \caption{Iteration 42}
    \label{fig:s1-iter-6}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/circ/loa-iter-50}
    \caption{Iteration 50}
    \label{fig:s1-iter-7}
  \end{subfigure}
  \caption{Benchmarking Function 1: $g1$}
\end{figure}

\par The second benchmarking function is:

$$
  g2=f(x_1, x_2) = x_1^2 + x_2^2
$$

and figure 29 shows the plots of the positions in 3d space with the z-space as function evaluations.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/sphr/loa-iter-0}
    \caption{Iteration 0}
    \label{fig:s2-iter-0}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/sphr/loa-iter-7}
    \caption{Iteration 7}
    \label{fig:s2-iter-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/sphr/loa-iter-14}
    \caption{Iteration 14}
    \label{fig:s2-iter-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/sphr/loa-iter-21}
    \caption{Iteration 21}
    \label{fig:s2-iter-3}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/sphr/loa-iter-28}
    \caption{Iteration 28}
    \label{fig:s2-iter-4}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/sphr/loa-iter-35}
    \caption{Iteration 35}
    \label{fig:s2-iter-5}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/sphr/loa-iter-42}
    \caption{Iteration 42}
    \label{fig:s2-iter-6}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/sphr/loa-iter-50}
    \caption{Iteration 50}
    \label{fig:s2-iter-7}
  \end{subfigure}
  \caption{Benchmarking Function 2: $g2$}
\end{figure}


\par The third benchmarking function is the Rastrigin benchmarking function for 1 dimension defined by

$$
  g3=f(x) = 10 + x^2 - 10 \cos(2\pi x)
$$

and figure 30 shows the plots of the positions on the 2d space with the y-space as function evaluations and graphs after every other iteration.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast1d/loa-iter-0}
    \caption{Iteration 0}
    \label{fig:s3-iter-0}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast1d/loa-iter-7}
    \caption{Iteration 7}
    \label{fig:s3-iter-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast1d/loa-iter-14}
    \caption{Iteration 14}
    \label{fig:s3-iter-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast1d/loa-iter-21}
    \caption{Iteration 21}
    \label{fig:s3-iter-3}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast1d/loa-iter-28}
    \caption{Iteration 28}
    \label{fig:s3-iter-4}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast1d/loa-iter-35}
    \caption{Iteration 35}
    \label{fig:s3-iter-5}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast1d/loa-iter-42}
    \caption{Iteration 42}
    \label{fig:s3-iter-6}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast1d/loa-iter-50}
    \caption{Iteration 50}
    \label{fig:s3-iter-7}
  \end{subfigure}
  \caption{Benchmarking Function 3: $g3$}
\end{figure}

\par The fourth benchmarking function is the Rastrigin benchmarking function for 2 dimensions defined by

$$
 g4= f(x_1, x_2) = 20 + x_1^2 - 10 \cos(2\pi x_1) + x_2^2 - 10 \cos(2\pi x_2)
$$

and figure 31 shows the plots of the positions after every other iteration on the xy-plane with z-space as function evaluations.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast2d/loa-iter-0}
    \caption{Iteration 0}
    \label{fig:s4-iter-0}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast2d/loa-iter-7}
    \caption{Iteration 7}
    \label{fig:s4-iter-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast2d/loa-iter-14}
    \caption{Iteration 14}
    \label{fig:s4-iter-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast2d/loa-iter-21}
    \caption{Iteration 21}
    \label{fig:s4-iter-3}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast2d/loa-iter-28}
    \caption{Iteration 28}
    \label{fig:s4-iter-4}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast2d/loa-iter-35}
    \caption{Iteration 35}
    \label{fig:s4-iter-5}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast2d/loa-iter-42}
    \caption{Iteration 42}
    \label{fig:s4-iter-6}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rast2d/loa-iter-50}
    \caption{Iteration 50}
    \label{fig:s4-iter-7}
  \end{subfigure}
  \caption{Benchmarking Function 4: $g4$}
\end{figure}


\par The fifth benchmarking function is the Rosenbrock benchmarking function for 2 dimensions defined by
$$
  g5=f(x_1, x_2) = (a-x_1)^2+b(x_2-x_1^2)^2
$$
with $a = 1$ and $b=100$ and figure 32 shows the plots of the positions on the xy-plane after every other iteration with the z-space as their function evaluations.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rosn2d-1-100/loa-iter-0}
    \caption{Iteration 0}
    \label{fig:s5-iter-0}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rosn2d-1-100/loa-iter-7}
    \caption{Iteration 7}
    \label{fig:s5-iter-1}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rosn2d-1-100/loa-iter-14}
    \caption{Iteration 14}
    \label{fig:s5-iter-2}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rosn2d-1-100/loa-iter-21}
    \caption{Iteration 21}
    \label{fig:s5-iter-3}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rosn2d-1-100/loa-iter-28}
    \caption{Iteration 28}
    \label{fig:s5-iter-4}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rosn2d-1-100/loa-iter-35}
    \caption{Iteration 35}
    \label{fig:s5-iter-5}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rosn2d-1-100/loa-iter-42}
    \caption{Iteration 42}
    \label{fig:s5-iter-6}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/smpl/rosn2d-1-100/loa-iter-50}
    \caption{Iteration 50}
    \label{fig:s5-iter-7}
  \end{subfigure}
  \caption{Benchmarking Function 5: $g5$}
\end{figure}


\par The sixth benchmarking function is the Griewank benchmarking function for 1 dimension defined by

$$
  g6=f(x) = 1 + \frac{1}{4000}x^2 - \cos(x)
$$

and figure 33 shows the plots of the positions on the x-axis after every other iteration with the y-space as function evaluations.

\begin{figure}
  \centering
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1d/loa-iter-0}
   \caption{Iteration 0}
   \label{fig:s6-iter-0}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1d/loa-iter-7}
   \caption{Iteration 7}
   \label{fig:s6-iter-1}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1d/loa-iter-14}
   \caption{Iteration 14}
   \label{fig:s6-iter-2}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1d/loa-iter-21}
   \caption{Iteration 21}
   \label{fig:s6-iter-3}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1d/loa-iter-28}
   \caption{Iteration 28}
   \label{fig:s6-iter-4}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1d/loa-iter-35}
   \caption{Iteration 35}
   \label{fig:s6-iter-5}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1d/loa-iter-42}
   \caption{Iteration 42}
   \label{fig:s6-iter-6}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1d/loa-iter-50}
   \caption{Iteration 50}
   \label{fig:s6-iter-7}
 \end{subfigure}
 \caption{Benchmarking Function 6: $g6$}
\end{figure}

\par The seventh benchmarking function is the Griewank benchmarking function for 2 dimension defined by

$$
  g7=f(x_1, x_2) = 1 + \frac{1}{4000}x_1^2 + \frac{1}{4000}x_2^2 - \cos(x_1) * \cos(\frac{1}{2}x_2\sqrt{2})
$$

and figure 34 shows the plots of the positions on the xy-plane after every other iteration with the z-space as function evaluations.

\begin{figure}
  \centering
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk2d/loa-iter-0}
   \caption{Iteration 0}
   \label{fig:s7-iter-0}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk2d/loa-iter-7}
   \caption{Iteration 7}
   \label{fig:s7-iter-1}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk2d/loa-iter-14}
   \caption{Iteration 14}
   \label{fig:s7-iter-2}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk2d/loa-iter-21}
   \caption{Iteration 21}
   \label{fig:s7-iter-3}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk2d/loa-iter-28}
   \caption{Iteration 28}
   \label{fig:s7-iter-4}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk2d/loa-iter-35}
   \caption{Iteration 35}
   \label{fig:s7-iter-5}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk2d/loa-iter-42}
   \caption{Iteration 42}
   \label{fig:s7-iter-6}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk2d/loa-iter-50}
   \caption{Iteration 50}
   \label{fig:s7-iter-7}
 \end{subfigure}
 \caption{Benchmarking Function 7: $g7$}
\end{figure}

\par The next function reintroduces $g1$ and offsets the global minima to x=2 and best fitness as 2. This function can be described by
$$
g8=f(x) = (x-2)^2 + 2
$$
and figure 35 shows the plot of the points in the x-axis with the y-space as function evaluations.

\begin{figure}
  \centering
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/circshft/loa-iter-0}
   \caption{Iteration 0}
   \label{fig:s8-iter-0}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/circshft/loa-iter-7}
   \caption{Iteration 7}
   \label{fig:s8-iter-1}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/circshft/loa-iter-14}
   \caption{Iteration 14}
   \label{fig:s8-iter-2}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/circshft/loa-iter-21}
   \caption{Iteration 21}
   \label{fig:s8-iter-3}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/circshft/loa-iter-28}
   \caption{Iteration 28}
   \label{fig:s8-iter-4}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/circshft/loa-iter-35}
   \caption{Iteration 35}
   \label{fig:s8-iter-5}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/circshft/loa-iter-42}
   \caption{Iteration 42}
   \label{fig:s8-iter-6}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/circshft/loa-iter-50}
   \caption{Iteration 50}
   \label{fig:s8-iter-7}
 \end{subfigure}
 \caption{Benchmarking Function 8: $g8$}
\end{figure}

\clearpage

\par The next 4 tests introduces the most of the previously described function in another interval.

\par The next test $t1$ reuses Rastrigin 1D function ($g3$) in the interval of $[\pi:2\pi]$ and figure 36 shows the plot of the points in x-axis and uses y-space for function evaluations.

\begin{figure}
  \centering
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast1dshft/loa-iter-0}
   \caption{Iteration 0}
   \label{fig:i1-iter-0}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast1dshft/loa-iter-7}
   \caption{Iteration 7}
   \label{fig:i1-iter-1}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast1dshft/loa-iter-14}
   \caption{Iteration 14}
   \label{fig:i1-iter-2}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast1dshft/loa-iter-21}
   \caption{Iteration 21}
   \label{fig:i1-iter-3}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast1dshft/loa-iter-28}
   \caption{Iteration 28}
   \label{fig:i1-iter-4}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast1dshft/loa-iter-35}
   \caption{Iteration 35}
   \label{fig:i1-iter-5}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast1dshft/loa-iter-42}
   \caption{Iteration 42}
   \label{fig:i1-iter-6}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast1dshft/loa-iter-50}
   \caption{Iteration 50}
   \label{fig:i1-iter-7}
 \end{subfigure}
 \caption{Interval test 1: $t1$}
\end{figure}

\par The next test $t2$ reuses Griewank 1D function ($g6$) in the interval of $[40:100]$ and figure 37 shows the plot of the points in x-axis and uses y-space for function evaluations.

\begin{figure}
  \centering
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1dshft/loa-iter-0}
   \caption{Iteration 0}
   \label{fig:i2-iter-0}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1dshft/loa-iter-7}
   \caption{Iteration 7}
   \label{fig:i2-iter-1}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1dshft/loa-iter-14}
   \caption{Iteration 14}
   \label{fig:i2-iter-2}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1dshft/loa-iter-21}
   \caption{Iteration 21}
   \label{fig:i2-iter-3}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1dshft/loa-iter-28}
   \caption{Iteration 28}
   \label{fig:i2-iter-4}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1dshft/loa-iter-35}
   \caption{Iteration 35}
   \label{fig:i2-iter-5}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1dshft/loa-iter-42}
   \caption{Iteration 42}
   \label{fig:i2-iter-6}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/grwnk1dshft/loa-iter-50}
   \caption{Iteration 50}
   \label{fig:i2-iter-7}
 \end{subfigure}
 \caption{Interval test 2: $t2$}
\end{figure}

\par The next test $t3$ reuses Rastrigin 2D function ($g4$) in the interval of $[\pi:2\pi]$ and figure 38 shows the plot of the points in xy-plane and uses z-space for function evaluations.

\begin{figure}
  \centering
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-0}
   \caption{Iteration 0}
   \label{fig:i3-iter-0}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-7}
   \caption{Iteration 7}
   \label{fig:i3-iter-1}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-14}
   \caption{Iteration 14}
   \label{fig:i3-iter-2}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-21}
   \caption{Iteration 21}
   \label{fig:i3-iter-3}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-28}
   \caption{Iteration 28}
   \label{fig:i3-iter-4}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-35}
   \caption{Iteration 35}
   \label{fig:i3-iter-5}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-42}
   \caption{Iteration 42}
   \label{fig:i3-iter-6}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-50}
   \caption{Iteration 50}
   \label{fig:i3-iter-7}
 \end{subfigure}
 \caption{Interval test 3: $t3$}
\end{figure}

\par The next test $t4$ reuses Griewank 2D function ($g7$) in the interval of $[\pi:5\pi]$ and figure 39 shows the plot of the points in xy-plane and uses z-space for function evaluations.

\begin{figure}
  \centering
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-0}
   \caption{Iteration 0}
   \label{fig:i4-iter-0}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-7}
   \caption{Iteration 7}
   \label{fig:i4-iter-1}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-14}
   \caption{Iteration 14}
   \label{fig:i4-iter-2}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-21}
   \caption{Iteration 21}
   \label{fig:i4-iter-3}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-28}
   \caption{Iteration 28}
   \label{fig:i4-iter-4}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-35}
   \caption{Iteration 35}
   \label{fig:i4-iter-5}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-42}
   \caption{Iteration 42}
   \label{fig:i4-iter-6}
 \end{subfigure}
 \begin{subfigure}[b]{0.4\textwidth}
   \includegraphics[width=\textwidth]{img/smpl/rast2dshft/loa-iter-50}
   \caption{Iteration 50}
   \label{fig:i4-iter-7}
 \end{subfigure}
 \caption{Interval test 4: $t4$}
\end{figure}

\par The global fitness achieved using each function is graphed against every iteration of the algorithm. This is displayed in Figures 40 and 41.
\par
\par The best and worst fitness achieved, along with the standard deviation and median, of each function after 10 runs of 50 iterations is presented in Table 3. In these runs, the interval used for functions $g1$-$g8$ was [-5,5].
\par

\begin{table}[h!]
\centering
\begin{tabular}{l c l c l c l c l}
\hline
 Function & Worst Fitness       & Best Fitness        & Standard Deviation          & Median        \\
\hline
g1   & 3.77e-10& 1.27e-14& 9.49e-11& 1.307e-11\\
\hline
g2  &1.92e-05 & 2.09e-08& 2.92e-06& 4.67e-06 \\
\hline
g3  &1.21e-06& 1.40e-12& 3.28e-07& 1.58e-07 \\
\hline
g4  &5.27e-01& 5.76e-04& 1.29e-01& 9.77e-03 \\
\hline
g5  &1.72e-02& 7.10e-05& 4.25e-03& 2.68e-03 \\
\hline
g6  &4.58e-11& 3.93e-14& 1.11e-11& 1.58e-13 \\
\hline
g7  &1.12e-04& 7.25e-08& 2.67e-05& 6.31e-05 \\
\hline
g8  &2.00 & 2.00 & 0.00 & 2.00 \\
\hline
t1  &1.36e+01& 1.36e+01& 0& 1.36e+01 \\
\hline
t2  &4.83e-01& 4.83e-01& 0& 4.83e-01 \\
\hline
t3  &2.71e+01& 2.71e+01& 0& 2.71e+01 \\
\hline
t4  &8.22e-03& 7.39e-03& 3.02e-04& 7.45e-03 \\
\hline
\end{tabular}
\caption{Fitness Results of Functions After 10 Runs}
\end{table}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/g1}
    \caption{g1}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/g2}
    \caption{g2}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/g3}
    \caption{g3}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/g4}
    \caption{g4}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/g5}
    \caption{g5}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/g6}
    \caption{g6}
  \end{subfigure}
  \caption{Run-Time vs Best Fitness Achieved of $g1$-$g6$}
\end{figure}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/g7}
    \caption{g7}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/g8}
    \caption{g8}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/t1}
    \caption{t1}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/t2}
    \caption{t2}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/t3}
    \caption{t3}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/summary/t4}
    \caption{t4}
  \end{subfigure}
  \caption{Run-Time vs Best Fitness Achieved of $g7$, $g8$, and $t1$-$t4$ }
\end{figure}

\clearpage

\section{Function Shifting, Scaling and Offsetting}

\par The fitness functions can be sometimes modified such that optimally finding solutions correctly according to the shift, scale or offset would prove the integrity of any algorithm. Some tests also use this technique to modify the interval available to a function by `scaling' a set of possible inputs inside the fitness function's domain.

\par A good example of the use of these techniques is a shifted rastrigin function. A typical rastrigin function has an n-dimensional domain of [-5.12, 5.12] for any dimension.

\par So, when using a standard input interval, it would require to rescale or sometimes shift the inputs. It would look something like this:

$$
  F(x) = f\left(\frac{s_f * x_0}{s_0} + o_x\right) + o_f
$$

where $F$ is the new function, $f$ is the original, $s_f$ is the scale of the new input interval, $s_0$ is the scale of the original interval, $o_x$ is the shift on the interval of new $x$ and $o_f$ is the offset of function fitness.

\par Here are some examples of before and after using some of these techniques.


% Example one parabola 1.1 shift 1.2 scale
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.6\textwidth}
    \includegraphics[width=\textwidth]{img/translates/parabola/loa-graph-orig}
    \caption{Original Parabola}
  \end{subfigure}
  \begin{subfigure}[b]{0.6\textwidth}
    \includegraphics[width=\textwidth]{img/translates/parabola/loa-graph-shift}
    \caption{Shifted Parabola}
  \end{subfigure}
  \begin{subfigure}[b]{0.6\textwidth}
    \includegraphics[width=\textwidth]{img/translates/parabola/loa-graph-scale}
    \caption{Scaled Parabola}
  \end{subfigure}
  \caption{Shift and Scale Parabola Example}
\end{figure}

\par Notice in Figure 42 the differences of the original to the shifted or scaled versions of the parabola function. The shift employed in this function is a $x$ shift of +20 and a fitness shift of +20 while the scaled version employs a $1/100^{th}$ $x$ input scale.

% Example two rastrigin 2.1 shift 2.2 scale
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.6\textwidth}
    \includegraphics[width=\textwidth]{img/translates/rastrigin/loa-graph-orig}
    \caption{Original Rastrigin}
  \end{subfigure}
  \begin{subfigure}[b]{0.6\textwidth}
    \includegraphics[width=\textwidth]{img/translates/rastrigin/loa-graph-shift}
    \caption{Shifted Rastrigin}
  \end{subfigure}
  \begin{subfigure}[b]{0.6\textwidth}
    \includegraphics[width=\textwidth]{img/translates/rastrigin/loa-graph-scale}
    \caption{Scaled Rastrigin}
  \end{subfigure}
  \caption{Shift and Scale Rastrigin Example}
\end{figure}

\par Notice in Figure 43 the difference between the original Rastrigin function to the shifted function, which uses a $x$ shift of +5.12 and a fitness shift of +5.12 (despite being slightly unnoticeable). Also the scaled Rastrigin can be compared to the original by the input interval it uses being [-100,100] from the original's [-5.12,5.12] in which the original is scaled to fit the [-100,100] but still producing the same fitness values on scale.

\clearpage

\section{CEC 2014 Benchmarks}

\par CEC is a committee known for creating competitions for optimization algorithms. Each year CEC provides a set of functions that are known for testing optimization algorithms for scientists to submit their work and compare the effectiveness of their algorithms. Each test suite includes functions that are shifted and rotated which are to be run in black box tests though their code is reviewable and written in C++.

\par The following parameters are set and used in the algorithm.

\begin{align*}
\text{Population} &= 50\\
\text{Number of Prides} &= 4\\
\text{Percent of Nomad Lions} &= 0.2\\
\text{Roaming Percent} &= 0.2\\
\text{Mutate Probability} &= 0.2\\
\text{Sex Rate} &= 0.8\\
\text{Mating Probability} &= 0.3\\
\text{Immigrate Rate} &= 0.4\\
\end{align*}

\par An overview of what functions are included in the test is seen in Table 4. f* in this table implies a shift of the function to have the said ideal fitness in the ideal global optima meaning that the function f1 in the table would have an ideal global optima having a fitness of about 100.

\clearpage

\begin{table}[]
\begin{tabular}{llll}
  \textbf{Type}
    & \textbf{ID} & \textbf{Function} & \textbf{f*} \\ \hline
  Unimodal
    & f1 & Rotated high conditioned elliptic function & 100 \\
    & f2 & Rotated bent cigar function & 200 \\
    & f3 & Rotated discus function & 300 \\
  Multimodal
    & f4 & Shifted and rotated Rosenbrock function & 400 \\
    & f5 & Shifted and rotated Ackley's function & 500 \\
    & f6 & Shifted and rotated Weierstrass function & 600 \\
    & f7 & Shifted and rotated Griewank's function & 700 \\
    & f8 & Shifted Rastrigin function & 800 \\
    & f9 & Shifted and rotated Rastrigin's function & 900 \\
    & f10 & Shifted Schwefel function & 1000 \\
    & f11 & Shifted and rotated Schwefel's function & 1100 \\
    & f12 & Shifted and rotated Katsuura function & 1200 \\
    & f13 & Shifted and rotated HappyCat function & 1300 \\
    & f14 & Shifted and rotated HGBat function & 1400 \\
    & f15 & Shifted and rotated Expanded Griewank's plus Rosenbrock's function & 1500 \\
    & f16 & Shifted and rotated Expanded Scaffer's F6 function & 1600 \\
  Hybrid
    & f17 & Hybrid function1 (f 9, f 8, f 1) & 1700 \\
    & f18 & Hybrid function2 (f 2, f 12, f 8) & 1800 \\
    & f19 & Hybrid function3 (f 7, f 6, f 4, f 14) & 1900 \\
    & f20 & Hybrid function4 (f 12, f 3, f 13, f 8) & 2000 \\
    & f21 & Hybrid function5 (f 14, f 12, f 4, f 9, f 1) & 2100 \\
    & f22 & Hybrid function6 (f 10, f 11, f 13, f 9, f 5) & 2200 \\
  Composition
    & f23 & Composition function1 (f 4, f 1, f 2, f 3, f 1) & 2300 \\
    & f24 & Composition function2 (f 10, f 9, f 14) & 2400 \\
    & f25 & Composition function3 (f 11, f 9, f 1) & 2500 \\
    & f26 & Composition function4 (f 11, f 13, f 1, f 6, f 7) & 2600 \\
    & f27 & Composition function5 (f 14, f 9, f 11, f 6, f 1) & 2700 \\
    & f28 & Composition function6 (f 15, f 13, f 11, f 16, f 1) & 2800 \\
    & f29 & Composition function7 (f 17, f 18, f 19) & 2900 \\
    & f30 & Composition function8 (f 20, f 21, f 22) & 3000
\end{tabular}
\caption{Functions defined by CEC 2014 and used for testing}
\end{table}

\clearpage

\par The following (Table 5) are results from running the reimplemented algorithm on the CEC 2014 Special Session and Competition on Real-Parameter Single Objective (Expensive) Optimization Benchmark\cite{liang_2013}, on a AMD Ryzen 5 2600 machine, parallelized on a 6 threaded workload to run 60 times of the algorithm on one function totalling to 30 functions. The limit placed to stop the algorithm is when it reaches 150,000 function evaluations. Each function ran for about 600 seconds on the machine. The best fitness for each run is recorded and computed for the Standard Deviation. The algorithm was written in MATLAB and the parallel computing toolbox was also used. The run times of the algorithm using each function were also graphed against the fitness values achieved at those times (Figures 44-47).


\begin{table}[]
\begin{tabular}{lllll}
    & \textbf{min}       & \textbf{max}       & \textbf{std}       & \textbf{median}    \\ \hline
f1  & 1.647E+07 & 6.730E+07 & 1.053E+07 & 3.844E+07 \\
f2  & 2.645E+05 & 6.888E+06 & 1.386E+06 & 2.092E+06 \\
f3  & 1.034E+04 & 3.092E+04 & 3.979E+03 & 1.670E+04 \\
f4  & 4.738E+02 & 6.284E+02 & 3.272E+01 & 5.611E+02 \\
f5  & 5.208E+02 & 5.210E+02 & 5.124E-02 & 5.210E+02 \\
f6  & 6.195E+02 & 6.296E+02 & 2.167E+00 & 6.254E+02 \\
f7  & 7.005E+02 & 7.011E+02 & 1.218E-01 & 7.009E+02 \\
f8  & 8.547E+02 & 9.298E+02 & 1.350E+01 & 8.845E+02 \\
f9  & 9.658E+02 & 1.028E+03 & 1.518E+01 & 9.971E+02 \\
f10 & 2.586E+03 & 7.135E+03 & 1.273E+03 & 3.864E+03 \\
f11 & 3.717E+03 & 8.245E+03 & 1.148E+03 & 7.470E+03 \\
f12 & 1.202E+03 & 1.203E+03 & 3.312E-01 & 1.203E+03 \\
f13 & 1.300E+03 & 1.301E+03 & 8.465E-02 & 1.300E+03 \\
f14 & 1.400E+03 & 1.400E+03 & 3.087E-02 & 1.400E+03 \\
f15 & 1.514E+03 & 1.549E+03 & 7.715E+00 & 1.525E+03 \\
f16 & 1.611E+03 & 1.613E+03 & 3.086E-01 & 1.612E+03 \\
f17 & 5.180E+05 & 3.363E+06 & 5.823E+05 & 1.612E+06 \\
f18 & 1.991E+03 & 1.350E+04 & 2.131E+03 & 2.910E+03 \\
f19 & 1.911E+03 & 1.923E+03 & 2.082E+00 & 1.919E+03 \\
f20 & 4.630E+03 & 2.635E+04 & 4.406E+03 & 1.484E+04 \\
f21 & 7.908E+04 & 7.593E+05 & 1.624E+05 & 2.698E+05 \\
f22 & 2.473E+03 & 3.134E+03 & 1.545E+02 & 2.782E+03 \\
f23 & 2.620E+03 & 2.633E+03 & 2.678E+00 & 2.624E+03 \\
f24 & 2.618E+03 & 2.632E+03 & 3.211E+00 & 2.627E+03 \\
f25 & 2.700E+03 & 2.709E+03 & 2.528E+00 & 2.707E+03 \\
f26 & 2.700E+03 & 2.701E+03 & 1.122E-01 & 2.700E+03 \\
f27 & 3.117E+03 & 3.739E+03 & 1.630E+02 & 3.536E+03 \\
f28 & 3.255E+03 & 4.404E+03 & 1.954E+02 & 3.296E+03 \\
f29 & 3.125E+03 & 4.300E+03 & 1.531E+02 & 3.131E+03 \\
f30 & 3.813E+03 & 7.945E+04 & 1.646E+04 & 4.796E+03
\end{tabular}
\caption{Results for LOA under CEC 2014 Benchmark}
\end{table}

\clearpage

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f1}
    \caption{f1}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f2}
    \caption{f2}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f3}
    \caption{f3}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f4}
    \caption{f4}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f5}
    \caption{f5}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f6}
    \caption{f6}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f7}
    \caption{f7}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f8}
    \caption{f8}
  \end{subfigure}
  \caption{Run-Time vs Fitness of Functions 1-8}
\end{figure}

\clearpage

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f9}
    \caption{f9}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f10}
    \caption{f10}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f11}
    \caption{f11}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f12}
    \caption{f12}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f13}
    \caption{f13}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f14}
    \caption{f14}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f15}
    \caption{f15}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f16}
    \caption{f16}
  \end{subfigure}
  \caption{Run-Time vs Fitness of Functions 9-16}
\end{figure}

\clearpage

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f17}
    \caption{f17}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f18}
    \caption{f18}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f19}
    \caption{f19}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f20}
    \caption{f20}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f21}
    \caption{f21}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f22}
    \caption{f 22}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f23}
    \caption{f23}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f24}
    \caption{f24}
  \end{subfigure}
  \caption{Run-Time vs Fitness of Functions 17-24}
\end{figure}

\clearpage

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f25}
    \caption{f25}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f26}
    \caption{f26}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f27}
    \caption{f27}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f28}
    \caption{f28}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f29}
    \caption{f29}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{img/cecrt/f30}
    \caption{f30}
  \end{subfigure}
  \caption{Run-Time vs Fitness of Functions 25-30}
\end{figure}

\clearpage


The original authors had results that are near and similar to the table except for f1, f2, f17 and 21 which had about 100 times or more fitter population than the reimplementations. The original paper was written such that there are gaps in logic that was filled by assumptions when the algorithm was being reimplemented. The original programming language used in the original implementation was not specified along with platform specifics which may result in test discrepancies. There also specifics that are key to how the algorithm works that were not defined by the original authors and tighter reimplementation may be needed. In another perspective, the reimplementation keeps up with some if not most algorithms that were used to compare with by the original authors. It also has had lesser deviation on some functions more than the original but also had some functions where its deviation was beat by the original.

\section{Conclusion}

As such, to conclude, the Lion Optimization Algorithm uses a Lion-Pride-Outcast behavior for a population to find solutions or what is thought of as a better territory or a safer place for the population to live in. The algorithm combines the effectiveness of swarm optimization in Prides and individual roaming that provides escaping optimas. It also harnesses Genetic Algorithm through mating in prides. The information exchange between Pride and Nomads is key for stuck prides to escape and for previous nomads to improve solutions.
